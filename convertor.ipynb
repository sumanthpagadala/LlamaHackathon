{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trl import SFTTrainer\n",
    "import bitsandbytes as bnb\n",
    "from safetensors.torch import load_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ed9b4b6d354b9db66cec1d816b603c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map=\"auto\")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"train.xlsx\")\n",
    "df = df.dropna()\n",
    "train_df, test_df = train_test_split(df, test_size=0.05, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "        \"self_attn.o_proj\",\n",
    "        \"self_attn.qkv_proj\",\n",
    "        \"mlp.gate_up_proj\",\n",
    "        \"mlp.down_proj\"\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(examples):\n",
    "    prompts = []\n",
    "    for text, gloss in zip(examples['English Text'], examples['BSL Gloss']):\n",
    "        prompt = f\"\"\"### SYSTEM: Translate the input text to British Sign Language (BSL) gloss accurately.\n",
    "\n",
    "### INPUT: {text}\n",
    "\n",
    "### OUTPUT:\n",
    "{gloss}\"\"\"\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        prompts,\n",
    "        max_length=100,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Set labels\n",
    "    model_inputs['labels'] = model_inputs['input_ids'].copy()\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9064bd7cb98841e6a33e0af5462e6f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train dataset:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07dd73f32e2143c689d20d8bf9f47bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test dataset:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    format_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Processing train dataset\"\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    format_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    "    desc=\"Processing test dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./exp_llama\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=1,   \n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,  \n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./Advance_llama\",\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=0.3,\n",
    "    group_by_length=True,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    # Added these arguments\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\trl\\trainer\\sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\trl\\trainer\\sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"input_ids\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msubbu27498\u001b[0m (\u001b[33msubbu27498-signapse\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\subbu\\Desktop\\FT-Llama-3.1\\wandb\\run-20241112_151905-srjtvqe4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/subbu27498-signapse/huggingface/runs/srjtvqe4' target=\"_blank\">./exp_llama</a></strong> to <a href='https://wandb.ai/subbu27498-signapse/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/subbu27498-signapse/huggingface' target=\"_blank\">https://wandb.ai/subbu27498-signapse/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/subbu27498-signapse/huggingface/runs/srjtvqe4' target=\"_blank\">https://wandb.ai/subbu27498-signapse/huggingface/runs/srjtvqe4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12baee8fb6744229a054b2f9ce692f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\bitsandbytes\\nn\\modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9522, 'grad_norm': 0.9316175580024719, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.18}\n",
      "{'loss': 3.8352, 'grad_norm': 1.037092685699463, 'learning_rate': 3.571428571428572e-05, 'epoch': 0.36}\n",
      "{'loss': 3.568, 'grad_norm': 1.3796314001083374, 'learning_rate': 5.3571428571428575e-05, 'epoch': 0.53}\n",
      "{'loss': 3.052, 'grad_norm': 1.2398269176483154, 'learning_rate': 7.142857142857143e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99d565e25a64867840e125fc978edc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6802444458007812, 'eval_runtime': 8.2448, 'eval_samples_per_second': 2.911, 'eval_steps_per_second': 2.911, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2965, 'grad_norm': 1.4498662948608398, 'learning_rate': 8.92857142857143e-05, 'epoch': 0.89}\n",
      "{'loss': 1.7857, 'grad_norm': 0.8467763066291809, 'learning_rate': 9.998445910004082e-05, 'epoch': 1.07}\n",
      "{'loss': 1.7488, 'grad_norm': 0.8459569811820984, 'learning_rate': 9.980973490458728e-05, 'epoch': 1.24}\n",
      "{'loss': 1.6877, 'grad_norm': 0.9724196195602417, 'learning_rate': 9.944154131125642e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeaf6dd3135e42d081a507ffe66225bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6528962850570679, 'eval_runtime': 8.2243, 'eval_samples_per_second': 2.918, 'eval_steps_per_second': 2.918, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5447, 'grad_norm': 1.1385596990585327, 'learning_rate': 9.888130844596524e-05, 'epoch': 1.6}\n",
      "{'loss': 1.2811, 'grad_norm': 1.0034748315811157, 'learning_rate': 9.81312123475006e-05, 'epoch': 1.78}\n",
      "{'loss': 1.4785, 'grad_norm': 0.5850300788879395, 'learning_rate': 9.719416651541839e-05, 'epoch': 1.96}\n",
      "{'loss': 1.2853, 'grad_norm': 0.6240348815917969, 'learning_rate': 9.607381059352038e-05, 'epoch': 2.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3fdda9d8d7475ea0b82169b5424b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4469184875488281, 'eval_runtime': 8.2037, 'eval_samples_per_second': 2.925, 'eval_steps_per_second': 2.925, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3757, 'grad_norm': 0.6302303671836853, 'learning_rate': 9.477449623286505e-05, 'epoch': 2.31}\n",
      "{'loss': 1.3283, 'grad_norm': 0.6537044644355774, 'learning_rate': 9.330127018922194e-05, 'epoch': 2.49}\n",
      "{'loss': 1.244, 'grad_norm': 0.6079965829849243, 'learning_rate': 9.165985472062246e-05, 'epoch': 2.67}\n",
      "{'loss': 1.2598, 'grad_norm': 0.5813496708869934, 'learning_rate': 8.985662536114613e-05, 'epoch': 2.84}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b038f162dc1345ac88104353dbd369ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.398092269897461, 'eval_runtime': 8.2225, 'eval_samples_per_second': 2.919, 'eval_steps_per_second': 2.919, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2996, 'grad_norm': 0.5394929051399231, 'learning_rate': 8.789858615727264e-05, 'epoch': 3.02}\n",
      "{'loss': 1.1464, 'grad_norm': 0.6827783584594727, 'learning_rate': 8.579334246298593e-05, 'epoch': 3.2}\n",
      "{'loss': 1.2629, 'grad_norm': 0.7005771398544312, 'learning_rate': 8.354907139929851e-05, 'epoch': 3.38}\n",
      "{'loss': 1.1324, 'grad_norm': 0.5817772150039673, 'learning_rate': 8.117449009293668e-05, 'epoch': 3.56}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8b0f44c0934c6c856bde2fa96703b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3714786767959595, 'eval_runtime': 8.2908, 'eval_samples_per_second': 2.895, 'eval_steps_per_second': 2.895, 'epoch': 3.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1027, 'grad_norm': 0.6218932271003723, 'learning_rate': 7.86788218175523e-05, 'epoch': 3.73}\n",
      "{'loss': 1.2019, 'grad_norm': 0.7266284823417664, 'learning_rate': 7.60717601689749e-05, 'epoch': 3.91}\n",
      "{'loss': 1.2309, 'grad_norm': 0.6880330443382263, 'learning_rate': 7.33634314136531e-05, 'epoch': 4.09}\n",
      "{'loss': 1.1647, 'grad_norm': 0.7678009867668152, 'learning_rate': 7.056435515653059e-05, 'epoch': 4.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0417bffed2b4bd1bddcd4724ef7417b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3612518310546875, 'eval_runtime': 8.189, 'eval_samples_per_second': 2.931, 'eval_steps_per_second': 2.931, 'epoch': 4.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0482, 'grad_norm': 0.847438633441925, 'learning_rate': 6.768540348112907e-05, 'epoch': 4.44}\n",
      "{'loss': 1.0623, 'grad_norm': 0.8038511872291565, 'learning_rate': 6.473775872054521e-05, 'epoch': 4.62}\n",
      "{'loss': 0.9994, 'grad_norm': 0.7134518027305603, 'learning_rate': 6.173287002338577e-05, 'epoch': 4.8}\n",
      "{'loss': 1.0987, 'grad_norm': 0.848036527633667, 'learning_rate': 5.868240888334653e-05, 'epoch': 4.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f8949b1db64c19acbf42ca1bfcb26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3559632301330566, 'eval_runtime': 8.2175, 'eval_samples_per_second': 2.921, 'eval_steps_per_second': 2.921, 'epoch': 4.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0012, 'grad_norm': 0.8271095752716064, 'learning_rate': 5.559822380516539e-05, 'epoch': 5.16}\n",
      "{'loss': 0.9803, 'grad_norm': 1.1144378185272217, 'learning_rate': 5.249229428303486e-05, 'epoch': 5.33}\n",
      "{'loss': 1.0762, 'grad_norm': 0.8909199237823486, 'learning_rate': 4.9376684270229254e-05, 'epoch': 5.51}\n",
      "{'loss': 0.9708, 'grad_norm': 1.0305852890014648, 'learning_rate': 4.626349532067879e-05, 'epoch': 5.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a44bb9eaf845b79dbeb2863b57b00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3701438903808594, 'eval_runtime': 8.2077, 'eval_samples_per_second': 2.924, 'eval_steps_per_second': 2.924, 'epoch': 5.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0106, 'grad_norm': 1.0250083208084106, 'learning_rate': 4.316481958449634e-05, 'epoch': 5.87}\n",
      "{'loss': 1.0249, 'grad_norm': 0.959048867225647, 'learning_rate': 4.0092692840030134e-05, 'epoch': 6.04}\n",
      "{'loss': 0.9135, 'grad_norm': 0.948573887348175, 'learning_rate': 3.705904774487396e-05, 'epoch': 6.22}\n",
      "{'loss': 0.9514, 'grad_norm': 1.164135456085205, 'learning_rate': 3.4075667487415785e-05, 'epoch': 6.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94502adc64ba4555bde0d97a71559705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4045311212539673, 'eval_runtime': 8.2425, 'eval_samples_per_second': 2.912, 'eval_steps_per_second': 2.912, 'epoch': 6.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9241, 'grad_norm': 1.2764636278152466, 'learning_rate': 3.115414001894974e-05, 'epoch': 6.58}\n",
      "{'loss': 0.9798, 'grad_norm': 1.1142703294754028, 'learning_rate': 2.8305813044122097e-05, 'epoch': 6.76}\n",
      "{'loss': 0.9475, 'grad_norm': 1.1603232622146606, 'learning_rate': 2.5541749944535554e-05, 'epoch': 6.93}\n",
      "{'loss': 0.7747, 'grad_norm': 1.0191832780838013, 'learning_rate': 2.2872686806712035e-05, 'epoch': 7.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58ab447ea6343bb87735bebb1cf07cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.409070611000061, 'eval_runtime': 8.2115, 'eval_samples_per_second': 2.923, 'eval_steps_per_second': 2.923, 'epoch': 7.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0325, 'grad_norm': 1.182223916053772, 'learning_rate': 2.0308990721324927e-05, 'epoch': 7.29}\n",
      "{'loss': 0.8542, 'grad_norm': 1.1488158702850342, 'learning_rate': 1.7860619515673033e-05, 'epoch': 7.47}\n",
      "{'loss': 0.8958, 'grad_norm': 1.0434894561767578, 'learning_rate': 1.553708307580265e-05, 'epoch': 7.64}\n",
      "{'loss': 0.9216, 'grad_norm': 1.2140082120895386, 'learning_rate': 1.33474064085087e-05, 'epoch': 7.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685e37ede8324482a1d88524f996e5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4040923118591309, 'eval_runtime': 8.1715, 'eval_samples_per_second': 2.937, 'eval_steps_per_second': 2.937, 'epoch': 7.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8167, 'grad_norm': 1.2580642700195312, 'learning_rate': 1.130009458668863e-05, 'epoch': 8.0}\n",
      "{'loss': 0.8262, 'grad_norm': 1.3295472860336304, 'learning_rate': 9.403099714207175e-06, 'epoch': 8.18}\n",
      "{'loss': 0.7745, 'grad_norm': 1.2822660207748413, 'learning_rate': 7.663790038585793e-06, 'epoch': 8.36}\n",
      "{'loss': 0.9262, 'grad_norm': 1.2643542289733887, 'learning_rate': 6.088921331488568e-06, 'epoch': 8.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b61d9ab7bc463790e561e3e0d7c9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4339872598648071, 'eval_runtime': 8.1696, 'eval_samples_per_second': 2.938, 'eval_steps_per_second': 2.938, 'epoch': 8.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subbu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\trl\\trainer\\sft_trainer.py:434\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m--> 434\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3518\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\accelerate\\accelerator.py:2192\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2193\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66b5c274785440d86eef2c1214d1d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded with 4-bit precision and LoRA adapter.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "checkpoint_path = \"exp_llama/checkpoint-180\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    config=config,\n",
    "    quantization_config=bnb_config \n",
    ")\n",
    "model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model successfully loaded with 4-bit precision and LoRA adapter.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:283\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Generate outputs\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m res \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m res\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\peft\\peft_model.py:1638\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1636\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1637\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1638\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1639\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1640\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\generation\\utils.py:1825\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[0;32m   1822\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[0;32m   1823\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[0;32m   1824\u001b[0m )\n\u001b[1;32m-> 1825\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43minputs_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1827\u001b[0m device \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m   1828\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_special_tokens(generation_config, kwargs_has_attention_mask, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:285\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example inference\n",
    "input_text = \"\"\"### SYSTEM: Translate the input text to British Sign Language (BSL) gloss. \n",
    "### INPUT: We provide digital sign language displays for clear, concise, and easy-to-understand visual information.\n",
    "\n",
    "\n",
    "### OUTPUT: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "# Generate outputs\n",
    "outputs = model.generate(\n",
    "    inputs_embeds=inputs,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "res = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b99195dd6943bc939b3c8f603dacf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'resize_tokens_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 24\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     20\u001b[0m     model_name, \n\u001b[0;32m     21\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     22\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mbnb_config \n\u001b[0;32m     23\u001b[0m )\n\u001b[1;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize_tokens_embeddings\u001b[49m(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[0;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, checkpoint_path)\n\u001b[0;32m     26\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'resize_tokens_embeddings'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# Load model and tokenizer\n",
    "checkpoint_path = r\"c:\\Users\\subbu\\Desktop\\Llama-3.1-train\\results\\checkpoint-540\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    config=config,\n",
    "    quantization_config=bnb_config \n",
    ")\n",
    "model.resize_tokens_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load soft prompts from checkpoint\n",
    "soft_prompt_path = \"./bsl_soft_prompt_model/checkpoint-145\"\n",
    "try:\n",
    "    state_dict = load_file(f\"{soft_prompt_path}/adapter_model.safetensors\")\n",
    "    soft_prompt_key = [k for k in state_dict.keys() if 'prompt_embeddings' in k][0]\n",
    "    soft_prompt = state_dict[soft_prompt_key]\n",
    "except:\n",
    "    try:\n",
    "        state_dict = torch.load(f\"{soft_prompt_path}/rng_state.pth\")\n",
    "        soft_prompt_key = [k for k in state_dict.keys() if 'prompt_embeddings' in k][0]\n",
    "        soft_prompt = state_dict[soft_prompt_key]\n",
    "    except:\n",
    "        raise ValueError(\"Could not load soft prompts from checkpoint\")\n",
    "\n",
    "# Ensure proper shape and device\n",
    "if len(soft_prompt.shape) == 2:\n",
    "    soft_prompt = soft_prompt.unsqueeze(0)  # [1, 50, 4096]\n",
    "soft_prompt = soft_prompt.to(device).to(torch.float16)\n",
    "print(f\"Loaded soft prompt with shape: {soft_prompt.shape}\")\n",
    "\n",
    "def generate_with_soft_prompt(model, tokenizer, input_text, soft_prompt, max_length=75):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get input embeddings\n",
    "    input_embeds = model.get_input_embeddings()(inputs['input_ids'])\n",
    "    \n",
    "    # Expand soft prompt to match batch size\n",
    "    batch_size = input_embeds.shape[0]\n",
    "    soft_prompt_expanded = soft_prompt.expand(batch_size, -1, -1)\n",
    "    \n",
    "    # Concatenate soft prompt with input embeddings\n",
    "    inputs_embeds = torch.cat([soft_prompt_expanded, input_embeds], dim=1)\n",
    "    \n",
    "    # Adjust attention mask\n",
    "    soft_prompt_mask = torch.ones(\n",
    "        (batch_size, soft_prompt.shape[1]),\n",
    "        device=inputs['attention_mask'].device,\n",
    "        dtype=inputs['attention_mask'].dtype\n",
    "    )\n",
    "    attention_mask = torch.cat([soft_prompt_mask, inputs['attention_mask']], dim=1)\n",
    "    \n",
    "    # Generate with embeddings\n",
    "    outputs = model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    \n",
    "    # Decode and return\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbca5e873d5044209e81a88f577f9d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WELCOME SIGNAPSE THIS SIGNAPSE\n",
      "GENERATIVE AI SIGNING TRANSLATION SOFTWARE CREATE SIGNING CREATE SIGNING\n",
      "SIGNING NEED VIDEO SIGNING HOW CREATE VIDEO SIGNING VIDEO SIGNING USE OUR AI TECHNOLOGY CREATE VIDEO SIGNING VIDEO HAVE CONTENT POINT-LEFT POINT-RIGHT POINT-RIGHT VIDEO SIGNING VIDEO SIGNING VIDEO HAVE VIDEO SIGNING VIDEO POINT-LEFT POINT-RIGHT POINT-RIGHT HAVE VIDEO SIGNING VIDEO POINT-LEFT POINT-RIGHT POINT-RIGHT HAVE VIDEO SIGNING VIDEO\n",
      "SIGNAPSE USE BIG VAST COLLECTION SIGNING VIDEO CREATE SIGNING PERSON TRANSLATOR WHAT SIGNAPSE SIGNING ACCURATE POINT-THIS POINT-THIS POINT-THIS POINT-THIS POINT-THIS POINT-THIS POINT-THIS POINT-THIS POINT-THIS POINT-THIS POINT-THIS POINT-THIS POINT-THIS POINT-THIS POINT-THIS POINT-THIS\n",
      "CLIENT HAVE TRUST WE DO OUR WORK ALSO TEAM HAVE HIGH AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE AIM-RANGE\n",
      "OUR TECHNOLOGY BASED COMPUTER VISION RESEARCH WORLD TOP UNIVERSITY SURREY CREATE\n",
      "YOU INTEREST JOIN OUR COMPANY CREATE SIGN TECHNOLOGY FUTURE\n",
      "SIGNAPSE TECHNOLOGY CAN SIGN TRANSLATE VIDEO PICTURE PICTURE SET-UP\n",
      "PUBLIC PLACE SIGNING TRANSLATION INFORMATION INFORMATION DEPARTURE DELAY CANCELLATION IN-MORE-DETAILS QUICK ACCESS\n",
      "MY POINTS OUR SOLUTION AMAZING NEW INFORMATION TRANSLATE QUICK ALP POINT-TWENTY SECONDS\n",
      "SIGNAPSE AIM NEXT AIM TRANSLATE SIGNING FULL TRANSLATION FULL SIGNING\n",
      "COMPETITIVE EQUITY PACKAGE WORK FROM HOME FULL REMOTE WORK ENVIRONMENT\n",
      "SIGNAPSE COMMITTED VALUE ALL MEMBER DIFFERENT NEED POINT-LEFT POINT-RIGHT POINT-RIGHT POINT-LEFT POINT-RIGHT POINT-LEFT POINT-RIGHT POINT-LEFT POINT-RIGHT POINT-LEFT POINT-RIGHT POINT-LEFT POINT-RIGHT POINT-LEFT POINT-RIGHT POINT-LEFT POINT-RIGHT POINT-LEFT POINT-RIGHT POINT-LEFT POINT-RIGHT\n",
      "WE AIM DEAF LEAD ALSO PARTNER DEAF ORGANISATION INCLUSION EQUALITY\n",
      "WE AIM ALL SOCIETY GROUP POINT-LEFT-POINT-RIGHT RESPECT ALL STAFF RESPECT VALUABLE\n",
      "WE PROVIDE DIGITAL SIGNING INFORMATION CLEAR CONCISE EASY-TO-READ INFORMATION INFORMATION\n",
      "TRANSPORT TRANSPORT SIGN TRANSPORT TRANSPORT SIGN TRANSPORT TRANSPORT SIGN TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT TRANSPORT\n",
      "OUR TECHNOLOGY ALP TRANSLATION ACCURATE EQUAL DEAF COMMUNITY ACCESS INFORMATION\n",
      "WE AIM MAKE SIGNING ACCESS FOR-ALL VIDEO PUBLIC ANNOUNCE TRANSLATE QUICKLY\n",
      "JOIN OUR TEAM DEVELOPER ENGINEER RESEARCHER CREATE NEW IDEA INNOVATION SIGNING VIDEO SIGNING TECHNOLOGY\n",
      "GENERATIVE AI TECHNOLOGY SIGNING VIDEO IN-VISION ALSO SIGNING GRAMMAR MATCH EXACT MATCH IN-VISION WHAT SIGNING VIDEO WATCH SIGNING VIDEO MATCH MATCH WHAT SIGNING VIDEO MATCH MATCH MATCH WHAT SIGNING VIDEO MATCH MATCH MATCH MATCH MATCH WHAT SIGNING VIDEO MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH\n",
      "WE VALUE CLIENTS INNOVATIVE FORWARD THINK FUTURE BUILD ACCESS SIGNING NEXT GENERATION\n",
      "ALSO IMPORTANT ACCESS SIGNING TRANSLATION FOR-ALL DEAF COMMUNITY LISTEN AUDIO ANNOUNCEMENT\n",
      "SIGNAPSE TECHNOLOGY INNOVATIVE SIGNING VIDEO WEBSITE PUBLIC AREA SIGNING TRANSLATION\n",
      "WE DEDICATED AIM YOUR COMPANY ACCESS DEAF PEOPLE THROUGH SIGNING SOFTWARE\n",
      "VIDEO TRANSLATE PLATFORM CHANGE WORLD ACCESSIBILITY DEAF PEOPLE LAUNCH SEPTEMBER TWENTY-TWENTY-FOUR HAVE A LOOK WHAT WE OFFER VIDEOS TRANSLATION SIGNING WILL LAUNCH SEPT TWENTY-FOUR-ONE\n",
      "WE VARIOUS DEAF HEARING PEOPLE WE WORK TOGETHER DEVELOP AI SIGNING TRANSLATION INNOVATION\n",
      "WE AIM AIM DRIVEN INCLUDE INCLUSION GOOD COMMUNICATION INNOVATION RESPECT GOOD COMMUNICATION\n",
      "WE AIM AIM EXPAND ACCESSIBILITY FOR DEAF PEOPLE THROUGH SIGNING TRANSLATION WHAT OUR AIM SIGNING ACCESSIBILITY SIGNING IMPORTANT\n",
      "OUR SIGNING SCRIPT POINT-TO-POINT TEXT TRANSLATE WEBSITE TEXT SIGNING SIGNING SIGNING WHAT SIGNING SIGNING SCRIPT POINT-TO-POINT SIGNING SIGNING SIGNING WHAT POINT-TO-POINT POINT-TO-POINT SIGNING SIGNING SIGNING SIGNING SIGNING POINT-TO-POINT POINT-TO-POINT POINT-TO-POINT\n",
      "JFK AIRPORT PARTNER WE ACCESSIBILITY SIGNING TRANSLATION WEBSITE ALL DEAF PEOPLE TRAVEL GO OUR WEBSITE\n",
      "PLATFORM MAKE-SURE SIGNING ACCESSIBILITY PROJECT IMPORTANT IMPORTANT IMPORTANT IMPORTANT IMPORTANT IMPORTANT IMPORTANT IMPORTANT IMPORTANT IMPORTANT IMPORTANT IMPORTANT IMPORTANT IMPORTANT IMPORTANT IMPORTANT\n",
      "WHAT OUR TEAM HEAD WHO HEAD ALP ENTREPRENEUR AI PERSON SIGNING TRANSLATION SPECIALIST\n",
      "WE MEET COVID LOCKDOWN MEET DISCUSS PROBLEMS WE NEED SIGNING TRANSLATION-ALP SOLUTION\n",
      "WE AIM AIM AIM INCLUDE INCLUSIVE DEAF LEADERSHIP EQUAL EQUAL REPRESENTATION\n",
      "ALSO IMPORTANT WHAT WE AIM GOOD QUALITY COMMUNICATION MESSAGE CLEAR CONNECT ALSO DELIVERY GOOD WHAT WE AIM GOOD\n",
      "WE WORLD CHANGE POINT-TO-LEFT HOW WE DO ACCESSIBILITY IMPROVE EXPERIENCE USER WE AIM POINT-TO-LEFT FORWARD ALWAYS\n",
      "RESPECT DRIVE ALL WE INTERACT VALUE CONTRIBUTE ETHICAL RIGHT ALRIGHT\n",
      "WHAT OUR AIM IMPORTANT COMMUNICATION TRANSPARENCY FAIRNESS RESPECT CONNECT RESPECT OUR AIM ALL OUR WORK\n",
      "WE AWESOME RECIPIENT UNIVERSITY SURREY VICE CHANCELLOR AWARD WOW VEE VEE CHANCELLOR AWARD-PLAQUE AMAZING\n",
      "OUR AIM INNOVATION WON POINT RECOGNISE WHAT Surrey BUSINESS AWARD\n",
      "DYNAMIC AWARD WINNER AIM AIM EXCELLENCE ALL-THE-TIME\n",
      "OUR TEAM PEOPLE DIFFERENT EXPERT TECHNOLOGY AI TRANSLATION ACCESSIBILITY\n",
      "BASHEER HAVE COMPUTER VISION MACHINE LEARNING EXPERTISE AI ALP IMPROVE-RANGE POINT-TO-POINT TRANSLATION\n",
      "LEIA BACKGROUND RAISE MONEY RESEARCH WHAT FINALLY CREATE MEANFUL IMPACT\n",
      "SUZIE FINANCE MAKE-SURE WE EXPAND FINANCE EXPENDITURE EXPAND OUR TECHNOLOGY SUSTAINABLE GROWTH\n",
      "RACHEL PERSON PERSONAL EXPERIENCE VAD DEAF BSL ACCESSIBILITY IMPORTANT ME IMPROVE-RANGE-OF-PRODUCTS-AND-SERVICES ME ME ME KNOW ACCESSIBILITY ME ME ME KNOW-THAT ME IMPROVE-RANGE-OF-PRODUCTS-AND-SERVICES ME IMPROVE-RANGE-OF-PRODUCTS-AND\n",
      "MARCUS MARCUS DEDICATION IMPROVE FIT OUR VISION IMPROVE DEAF PEOPLE LIFE\n",
      "MARCELS FRONT-END DEVELOPMENT USEFUL USER FRIENDLY INTERFACE\n",
      "ZOE PRODUCT MANAGE PRODUCT PRODUCT EFFICIENT RELIABLE PRODUCT PRODUCT PRODUCT\n",
      "ALEXANDRU ENGINEERING SKILL ACCESSIBILITY FRONT-END PLATFORM IMPROVE FUNCTIONALITY\n",
      "PEARSE HEAD INVESTMENT BOARD POINT-LEFT SUPPORT OUR GROWTH STRATEGY DECISION PLAN-RIGHT\n",
      "MARCUS DIVERSITY EXPERTISE ADD DEAF HEARING COMMUNITY OUR SERVICE\n",
      "AI USE OUR COMPANY SIGNING USE SIGNING ACCESSIBILITY WORLD WORLD INCLUSIVE IMPROVE WORLD SIGNING ACCESSIBILITY WORLD SHOULD BE ACCESSIBLE FOR-ALL WE WORLD IMPROVE WORLD SIGNING ACCESSIBILITY WORLD SHOULD BE ACCESSIBLE FOR-ALL WE WORLD IMPROVE WORLD IMPROVE WORLD ACCESSIBILITY WORLD SHOULD BE ACCESSIBLE FOR-ALL WE WORLD IMPROVE WORLD\n",
      "WE IMPROVE DEAF PEOPLE SUMMARY HOW WE HELP DEAF PEOPLE USE INFORMATION ACCESS FULL DIFFERENT VIDEO SIGNING STORY IN-VIDEO SIGNING STORY IN-PAGE SIGNING STORY VARIETY-VARIETY-VARIETY\n",
      "JOIN OUR TEAM CREATE SIGNING FUTURE FUTURE AI SIGNING TRANSLATION\n",
      "WE USE AI TECHNOLOGY SIGNING PROCESS INTEGRATE NATURAL SMOOTH CHANGE FROM SPEECH SIGNING\n",
      "WE CREATE SIGNING VIDEO WEBSITE ANNOUNCE SIGNING ACCESSIBILITY IMPROVE ACCESSIBILITY FOR-ALL-DISABILITY ACCESSIBILITY IMPROVE ACCESSIBILITY FOR-ALL-DISABILITY\n",
      "RACHEL EXPERIENCE DEAF AMAZING THAT-IS WHY WE HAVE ACCESSIBILITY AIM IMPROVE IMPROVE EQUALITY\n",
      "STACEY MARKETING SKILL EXPAND OUR SERVICE ALL PEOPLE SIGNING\n",
      "TRANSPORT TRANSPORT TRAIN TRANSPENNINE EXPRESS WHAT INCLUSIVE BSL SIGNING BOARD THAT USEFUL FOR-ALL PUBLIC ACCESSIBILITY\n",
      "WE CONNECT COLLECTIVE PARTNER CULTURE SIGNING OWN CULTURE OWN SIGNING COMMUNITY\n",
      "OUR TEAM AIM ELIMINATE DISCRIMINATION PROMOTE DIVERSITY OUR WORK OUR WORK EVERYTHING\n",
      "INNOVATION OUR CORE AIM CONTINUOUS IMPROVE USER EXPERIENCE\n",
      "OUR AIM EXPAND SIGNING ACCESSIBILITY ALP DEAF HEARING STAFF CONNECT CONNECT RESONATE BOTH-SIDED EXCITED\n",
      "FSE GROUP INVEST VISION SIGNING TECHNOLOGY ACCESSIBILITY IMPROVE\n",
      "EMPOWER DEAF PEOPLE SIGNING SIGNING TRANSLATION OUR COMPANY WHAT IMPORTANT IMPORTANT IMPORTANT IMPORTANT\n",
      "FUTURE GENERATION CREATE SIGNING ACCESS IMPORTANT COMPANY COLLABORATION\n",
      "WE AIM EQUALITY BUSINESS PARTNERSHIP THAT IMPROVE SOCIETY CHANGE\n",
      "SIGNAPSE SIGNING AI CHANGE ACCESSIBILITY HOW ACCESSIBILITY SIGNING SCRIPT CREATE SIGNING SCRIPT ACCESSIBLE FOR-ALL VIDEO TRANSLATION VIDEO SCRIPT SIGNING SCRIPT CREATE SIGNING ACCESSIBILITY FOR-ALL ACCESSIBLE SIGNING SCRIPT CREATE SIGNING ACCESSIBILITY FOR-ALL ACCESSIBLE SIGNING SCRIPT CREATE WHAT ACCESSIBILITY SIGNING SCRIPT CREATE SIGNING SCRIPT ACCESSIBLE FOR-ALL\n",
      "WE HAVE CHANGE PUBLIC PLACE SIGNING IMPROVE SIGNING ACCESS CONNECT PEOPLE HEARING-LOSS IMPROVE COMMUNITY CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT\n",
      "IMPROVE SIGNING PUBLIC TRANSPORT IMPORTANT DEAF PEOPLE INDEPENDENT-LY ACCESS TRANSPORTATION\n",
      "OUR PRODUCT BRIDGE INFORMATION GAP SIGNING INFORMATION INFORMATION POINT-TO-PRESENT-TIMING ACCURACY\n",
      "CREATE WORLD INCLUSIVE WORLD NEED SIGNING TRANSLATION CREATE MEANING SIGNING TRANSLATION FOR-ALL ACCESS EQUAL ACCESS\n",
      "WE AIM EXCEL OUR TECHNOLOGY SIGNING EASY CONNECT SIGNING TRANSLATION\n",
      "WE CHAMPION DEAF LEADERSHIP WE FIGHT FOR EQUAL OPPORTUNITY ALL OUR WHAT WE DO\n",
      "WE AIM AIM IMPROVE ACCESSIBILITY SIGNING NEW SIGNING SOLUTIONS INNOVATION\n",
      "WE AIM PROGRESS IMPROVE BOTH CLIENT PARTNER WE ALWAYS OPEN TRANSPARENT OUR WORK FACTS ALSO QUALITY WE KEEP-UP IMPROVE OUR WORK WE ALSO KEEP-UP INTERACTION OUR CLIENT PARTNER FACTS ALSO QUALITY\n",
      "FINANCE IMPORTANT ACCESSIBLE MANAGE FUTURE GROWTH SUCCESSFUL WORK ACHIEVE\n",
      "OUR AIM IMPROVE TECHNOLOGY ACCESSIBILITY FOR-ALL ACCESSIBILITY IMPORTANT IMPORTANT ACCESSIBILITY FOR-ALL\n",
      "WE AIM AIM IMPROVE OUR AI PRODUCT OUR AIM SET HIGH SET STANDARD SIGNING TRANSLATION\n",
      "DIGITAL SIGNING INFORMATION ACCESS FOR-ALL INFORMATION DISPLAY FOR-ALL ACCESSIBILITY CHANGE SIGNING DIGITAL MUST HAVE INFORMATION FOR-ALL\n",
      "PARTNER BRAND WORLD WE WIDE INDUSTRY TRANSLATION SIGNING\n",
      "OUR PRODUCT PUT VIDEO SIGNING EASY EASY READ ACCESS FOR-ALL ACCESSIBILITY IMPORTANT IMPORTANT IMPORTANT\n",
      "TRANSLATION: TRANSLATE REAL-TIME ACCESSIBILITY MULTIPLE NEED YOUR COMMUNICATE\n",
      "AI USE TECHNOLOGY USE ME ME INCLUDE IMPORTANT INCLUDE ACCESS IMPORTANT ACCESS EQUAL ACCESS INCLUDE INCLUSIVE EQUAL IMPORTANT ACCESS ACCESS EQUAL ACCESS EQUAL ACCESS ACCESS EQUAL ACCESS ACCESS EQUAL IMPORTANT ACCESS ACCESS EQUAL ACCESS ACCESS EQUAL ACCESS EQUAL ACCESS IMPORTANT ACCESS EQUAL ACCESS EQUAL ACCESS IMPORTANT ACCESS EQUAL ACCESS EQUAL ACCESS EQUAL ACCESS IMPORTANT EQUAL ACCESS ACCESS EQUAL ACCESS EQUAL ACCESS IMPORTANT EQUAL ACCESS EQUAL ACCESS EQUAL ACCESS IMPORTANT EQUAL ACCESS EQUAL ACCESS\n",
      "PARTNER AIRPORT TRAIN STATION TRANSPORT WE IMPROVE SIGNING ANIMATION TRAVEL DEAF PEOPLE ACCESS INFORMATION EQUAL ACCESS FULL ACCESS CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT CONNECT\n",
      "DYNAMIC TRANSLATION TRAIN STATION AIRPORT IMPROVE ACCESS ALL PEOPLE TRAVEL TRAVEL TRANSPORT TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION TRANSPORTATION\n",
      "AI USE SIGNING TRANSLATE EASY QUICK CORRECT\n",
      "WE USE NEW TECHNOLOGY CONNECT DEAF PEOPLE WORLD CONNECT\n",
      "SYSTEM: TRANSLATE INPUT TO BSL TRANSLATION\n",
      "\n",
      "### INPUT: AT POINT-TO-POINT TRANSPORTATION STOPS\n",
      "\n",
      "### OUTPUT:\n",
      "TRAINS TRAINS TRAIN STATION TRAIN STATION BUS TRAINS TRAIN STATION TRAINS TRAIN STATION TRAINS TRAINS TRAINS TRAIN TRAIN TRAIN TRAIN TRAIN TRAIN TRAIN TRAIN TRAIN TRAIN TRAIN TRAIN TRAIN TRAIN TRAIN TRAIN TRAIN\n",
      "WE MAKE-SURE SIGNING VIDEO REALISTIC ACCURATE SIGNING WE AIM POINT-TO-POINT ENGAGE USER CLEAR SIGNING\n",
      "DELIVER GOOD SIGNING POINT-TO-POINT SIGNING TRANSLATION EXCELLENCE IMPORTANT HIGH STANDARDS ALL CONTENT MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH MATCH\n",
      "TRANSPARENT PROCESS WHAT WE DO INTEGRITY HONESTY ALL OUR INTERACTION\n",
      "DRIVING CHANGE ACCESSIBILITY SOLUTIONS CREATE INCLUSIVE WORLD FOR-ALL WE AIM CREATE INCLUSIVE WORLD FOR-ALL\n",
      "PLATFORM OUR USER FRIENDLY EASY NAVIGATION ACCESSIBILITY EASY USE\n",
      "SIGNAPSE AIM DIFFERENT TYPE EQUALITY IMPORTANT HOW SET-UP PROJECT PARTNERSHIP-MULTIPLE\n",
      "WE IMPROVE SIGNING THROUGH FEEDBACK FROM YOU WE CONTINUOUS IMPROVE OUR SIGNING SERVICE IMPROVE SIGNING SIGNING SERVICE\n",
      "COLLABORATE INDUSTRY COMPANY WE IMPROVE ACCESS SIGNING DIFFERENT COMPANY DIFFERENT FIELD AREA\n",
      "Translation completed. Results saved to test_with_translations.csv\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    "import pandas as pd\n",
    "def process_csv(input_file, output_file, text_column='Text'):\n",
    "    \"\"\"\n",
    "    Process a CSV file and add BSL gloss translations\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input CSV file\n",
    "        output_file (str): Path to save output CSV file\n",
    "        text_column (str): Name of the column containing English text\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    # Create a new column for translations\n",
    "    df['Translated_glosses'] = ''\n",
    "    \n",
    "    # Process each row\n",
    "    for idx in tqdm(range(len(df)), desc=\"Translating\"):\n",
    "        text = df.iloc[idx][text_column]\n",
    "        \n",
    "        # Skip empty or invalid entries\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            continue\n",
    "            \n",
    "        # Prepare input text\n",
    "        input_text = f\"\"\"### SYSTEM: Translate the input text to British Sign Language (BSL) gloss. \n",
    "\n",
    "### INPUT: {text}\n",
    "\n",
    "### OUTPUT: \n",
    "\"\"\"\n",
    "        \n",
    "        # Generate translation\n",
    "        result = generate_with_soft_prompt(model, tokenizer, input_text, soft_prompt)\n",
    "        \n",
    "        # Extract translated text\n",
    "        try:\n",
    "            translated_text = result.split(\"OUTPUT:\")[-1].strip()\n",
    "            print(result)\n",
    "            df.at[idx, 'Translated_glosses'] = translated_text\n",
    "        except:\n",
    "            print(f\"Error processing row {idx}\")\n",
    "            df.at[idx, 'Translated_glosses'] = ''\n",
    "            \n",
    "        # Save progress periodically\n",
    "        if idx % 100 == 0:\n",
    "            df.to_csv(output_file, index=False)\n",
    "            \n",
    "    # Save final result\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Translation completed. Results saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = \"test.xlsx\"\n",
    "output_file = \"test_with_translations.csv\"\n",
    "process_csv(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated BSL gloss:\n",
      "ALSO AI TECHNOLOGY MATCH SIGNING VIDEO WHICH MATERIAL INPUT CONTENT-MATERIAL\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_text = \"\"\"### SYSTEM: Translate the input text to British Sign Language (BSL) gloss. \n",
    "\n",
    "### INPUT: Our AI technology chooses a combination of sign language videos based on the content provided.\n",
    "\n",
    "### OUTPUT: \n",
    "\"\"\"\n",
    "\n",
    "# Generate with soft prompts\n",
    "result = generate_with_soft_prompt(model, tokenizer, input_text, soft_prompt)\n",
    "print(\"\\nGenerated BSL gloss:\")\n",
    "output_text = result.split(\"OUTPUT:\")[-1].strip()\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
